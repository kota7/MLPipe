% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlp.R
\name{mlp}
\alias{MLP}
\alias{mlp}
\alias{mlp_classifier}
\alias{mlp_regressor}
\title{Multilayer perceptron}
\description{
Multilayer perceptron
}
\section{Usage}{

\preformatted{
mlp(output = 'sigm', hidden_sizes = 10, activation = 'sigm',
  learn_rate = 0.9, learn_rate_decay = 1, momentum = 0.5,
  num_epoch = 5, batch_size = 100,
  hidden_dropout = 0, visible_dropout = 0)

mlp_classifier(hidden_sizes = 10, activation = 'sigm',
  learn_rate = 0.9, learn_rate_decay = 1, momentum = 0.5,
  num_epoch = 5, batch_size = 100,
  hidden_dropout = 0, visible_dropout = 0)

mlp_regressor(hidden_sizes = 10, activation = 'sigm',
  learn_rate = 0.9, learn_rate_decay = 1, momentum = 0.5,
  num_epoch = 5, batch_size = 100,
  hidden_dropout = 0, visible_dropout = 0)}
}

\section{Arguments}{

\describe{
\item{\code{output}}{output unit form. \code{'sigm'}, \code{'linear'} or \code{'softmax'}}
\item{\code{hidden_sizes}}{integer vector of hidden unit sizes}
\item{\code{activation}}{activation function. \code{'sigm'}, \code{'tanh'} or \code{'linear'}}
\item{\code{learn_rate}}{learning rate}
\item{\code{learn_rate_decay}}{scale multipled to learning rate after each iteration}
\item{\code{momentum}}{momentum for gradient descent}
\item{\code{num_epoch}}{number of iteration}
\item{\code{batch_size}}{mini-batch size}
\item{\code{hidden_dropout}}{drop out fraction for hidden layer}
\item{\code{visible_dropout}}{drop out fraction for input layer}
}
}

\section{Value}{

\code{MLP} class object
}

\section{Class Methods}{

\describe{
\item{\preformatted{fit(x, y)}}{train neural network}
\item{\preformatted{predict(x, y = NULL)}}{return predicted values}
\item{\preformatted{incr_fit(x, y)}}{train neural network incrementally}
}
}

\section{Details}{

Uses \code{\link[deepnet]{nn.train}} as the backend.
\code{fit} method trains the network from the scratch; Use \code{incfit} method for incremental learning.
}
\examples{
set.seed(123)
# example from \\code{\\link{deepnet::nn.train}}
Var1 <- c(rnorm(50, 1, 0.5), rnorm(50, -0.6, 0.2))
Var2 <- c(rnorm(50, -0.8, 0.2), rnorm(50, 2, 1))
x <- matrix(c(Var1, Var2), nrow = 100, ncol = 2)
y <- c(rep(1, 50), rep(0, 50))
m <- mlp(hidden_sizes=5, learn_rate=0.8, num_epoch=3)
m$fit(x, y)

# classification example
data(iris)
x <- iris[,-5]
y <- iris[,5]
tr <- c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25))
m <- mlp_classifier(num_epoch=300)
m$fit(x[tr,], y[tr])
table(y[-tr], m$predict(x[-tr,]))

\dontrun{
# regression example
n <- 1000
x <- runif(2*n)
dim(x) <- c(n, 2)
y <- pmin(x[,1], x[,2])
m <- mlp_regressor(hidden_sizes=c(10), num_epoch=500, batch_size=25)
m$fit(x, y)
newx <- expand.grid(x1=seq(0, 1, length=50), x2=seq(0, 1, length=50))
pred <- m$predict(newx)
true <- pmin(newx[,1], newx[,2])
cor(true, pred)
dim(pred) <- c(50, 50)
dim(true) <- c(50, 50)
par(mfrow=c(1, 2))
contour(true)
contour(pred)}
}

